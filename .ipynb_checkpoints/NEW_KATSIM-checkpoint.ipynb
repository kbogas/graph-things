{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Drug-Drug side effects prediction\n",
    "\n",
    "In this notebook, we will face the problem of predicting the side effects of drugs from a Tensor Factorization perspective. Goal of this report is to examine the use of Tensor Networks in biomedical data.\n",
    " \n",
    "\n",
    "## Datasets\n",
    "\n",
    "As our dataset we use a multirelational network of drug-drug interactions, where each interaction represents a side effect type. In total we have 645 drugs and 1317 different side effects. The dataset used can be found in snap.stanford.edu/decagon . \n",
    "\n",
    "## Load dataset and statistics\n",
    "\n",
    "Here we present some basic statistics derived from the dataset. \n",
    "\n",
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "import t3f\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from numpy import dot, array, zeros, setdiff1d\n",
    "from numpy.random import shuffle\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from sktensor import dtensor, cp_als, tucker, rescal, ktensor\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score, precision_recall_fscore_support\n",
    "from tensorly import tucker_to_tensor, kruskal_to_tensor, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data and convert the .csv files to adjacency matrices and then to tensors (we only use drug-drug networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (1,2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: unorderable types: int() < str(), sort order is undefined for incomparable objects\n"
     ]
    }
   ],
   "source": [
    "def load_csvfile(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "def convert_data_to_adjMatrix(data):\n",
    "    # TODO:: refactor how to get columns\n",
    "    columns = (data.iloc[:, i] for i in range(2))\n",
    "    df = pd.crosstab(*columns)\n",
    "    #df = pd.crosstab(data.a, data.b)\n",
    "    idx = df.columns.union(df.index)\n",
    "    adj_matrix = df.reindex(index=idx, columns=idx, fill_value=0)\n",
    "    return adj_matrix.values\n",
    "\n",
    "def convert_dataframe_to_sparse(matrix):\n",
    "    return sp.csr_matrix(matrix)\n",
    "\n",
    "data_directory = \"./data/\"\n",
    "filenames = [\"bio-decagon-ppi.csv\",\n",
    "                 \"bio-decagon-targets.csv\",\n",
    "                 \"bio-decagon-targets-all.csv\",\n",
    "                 \"bio-decagon-combo.csv\",\n",
    "                 \"merged_data.csv\"]\n",
    "datasets = {}\n",
    "for f in filenames:\n",
    "    datafile = load_csvfile(data_directory+f)\n",
    "    adj_matrix = convert_data_to_adjMatrix(datafile)\n",
    "    sparse_matrix = convert_dataframe_to_sparse(adj_matrix)\n",
    "    datasets[f] = sparse_matrix          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have loaded our datasets in memory. Let's calculate some statistics for each network. We count the number of relations and the sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_counts(dist, title=\"\", x_label=\"\", y_label=\"\"):\n",
    "    plt.figure(figsize=(6, 3.5))\n",
    "    plt.plot(dist)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.title(\"Counts of interactions\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution(dist, title=\"\", x_label=\"\", y_label=\"\"):\n",
    "    print(\"Median:\", np.median(dist))\n",
    "    plt.figure(figsize=(6, 3.5))\n",
    "    sns.set_context(\"paper\", font_scale=1.8)\n",
    "    sns.set_style('ticks')\n",
    "    sns.set_style({\"xtick.direction\": \"in\", \"ytick.direction\": \"in\"})\n",
    "    sns.distplot(dist, kde=False, color=sns.xkcd_rgb['blue'], bins=30, hist_kws={\"alpha\" : 0.7})\n",
    "    plt.xlabel(x_label)\n",
    "    plt.title(\"Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "def plot_sparsity(dist):\n",
    "    print(dist.shape)\n",
    "    plt.figure()\n",
    "    plt.title(\"Sparsity\")\n",
    "    plt.spy(dist, precision=0.5, markersize=5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_matrix_stats(matrix):\n",
    "    # count nonzero values\n",
    "    nnz_count = len(matrix.data)\n",
    "    print(\"Nonzero values: \", nnz_count)\n",
    "    # sparsity of matrix\n",
    "    sparsity = nnz_count / float(matrix.shape[0]**2)\n",
    "    print(\"Sparsity: \", sparsity)\n",
    "    # count max interactions\n",
    "    count_nnz_matrix = matrix.getnnz(axis=1)\n",
    "    print(count_nnz_matrix.shape)\n",
    "    #print type(count_nnz_matrix)\n",
    "    #print count_nnz_matrix\n",
    "    #print count_nnz_matrix.max(axis=0)\n",
    "    row_max = [i\n",
    "                  for i in range(count_nnz_matrix.shape[0])\n",
    "                  if count_nnz_matrix[i] == count_nnz_matrix.max(axis=0)]\n",
    "    print(\"Row with maximum counts: \",  row_max)\n",
    "\n",
    "    # find topN interactions with indices\n",
    "    N = 3\n",
    "    topN_indices = np.argpartition(count_nnz_matrix, -N)[-N:]\n",
    "    #print topN_indices\n",
    "    # values of topN items\n",
    "    #print count_nnz_matrix[topN_indices]\n",
    "\n",
    "    #\n",
    "    plot_sparsity(matrix)\n",
    "    plot_counts(count_nnz_matrix,\n",
    "                      \"\",\n",
    "                      \"node\",\n",
    "                      \"Number of interactions\")\n",
    "    plot_distribution(count_nnz_matrix,\n",
    "                      \"\",\n",
    "                      \"Number of nodes\",\n",
    "                      \"Number of interactions\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plots for the \"bio-decagon-ppi.csv\" are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_matrix_stats(datasets[\"bio-decagon-ppi.csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plots for the \"bio-decagon-targets.csv\" are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_matrix_stats(datasets[\"bio-decagon-targets.csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plots for the \"bio-decagon-targets-all.csv\" are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_matrix_stats(datasets[\"bio-decagon-targets-all.csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plots for the \"bio-decagon-combo.csv\" are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_matrix_stats(datasets[\"bio-decagon-combo.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_matrix_stats(datasets[\"merged_data.csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the input data is a matrix, this result does not reflect all relationships in the network. We only assume one interaction between drugs for all relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include all relationships we have to build a tensor for drug-drug relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_multiarray(data):\n",
    "    \"\"\"\n",
    "    Convert data to a multiway array by stacking matrix slices\n",
    "    (also sparse version)\n",
    "    \"\"\"\n",
    "    adj_matrices = []\n",
    "    adj_sp_matrices = []\n",
    "    unique_seffects = data.iloc[:, 2].unique()\n",
    "    # build a huge zeros matrix for all nodes\n",
    "    columns = (data.iloc[:, i] for i in range(2))\n",
    "    df = pd.crosstab(*columns)\n",
    "    #df = pd.crosstab(data.a, data.b)\n",
    "    idx = df.columns.union(df.index)\n",
    "    adj_matrix_all = df.reindex(index=idx, columns=idx, fill_value=0)\n",
    "\n",
    "    # for each disease get all interaction rows\n",
    "    # and construct a DxDx#unique_seffects\n",
    "    #print len(unique_seffects.tolist())\n",
    "\n",
    "    for se in unique_seffects.tolist():\n",
    "        adj_matrix_all[adj_matrix_all>0] = 0\n",
    "        subdata = data.loc[data.iloc[:, 2] == se]\n",
    "        #columns = (subdata.iloc[:, i] for i in range(3))\n",
    "        for c in subdata.values:\n",
    "            row = c[0]\n",
    "            column = c[1]\n",
    "            adj_matrix_all.at[row, column] = 1\n",
    "        adj_matrices.append(adj_matrix_all.values)\n",
    "        sparse_matrix = convert_dataframe_to_sparse(adj_matrix_all)\n",
    "        adj_sp_matrices.append(sparse_matrix)\n",
    "    #multiarray = np.dstack(adj_matrices)\n",
    "\n",
    "    return adj_sp_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = load_csvfile(data_directory+\"bio-decagon-combo.csv\")\n",
    "multiarray = convert_data_to_multiarray(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with Tensor Factorization\n",
    "\n",
    "We will consider the side effect prediction problem as a link prediction. For this reason we employ tensor factorization approaches to tackle it.\n",
    "\n",
    "### Methods used\n",
    "\n",
    "We use:\n",
    "\n",
    "* Rescal\n",
    "* Canonical\n",
    "* Tucker\n",
    "* Tensor-Train\n",
    "\n",
    "We also plan to test supevised tensor decomposition techniques. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK = 5\n",
    "\n",
    "def predict_rescal_als(T, isClass=False):\n",
    "    A, R, _, _, _ = rescal.als(\n",
    "        T, 100, init='nvecs', conv=1e-3,\n",
    "        lambda_A=10, lambda_R=10\n",
    "    )\n",
    "    n = A.shape[0]\n",
    "    if isClass:\n",
    "        return A, R\n",
    "    P = zeros((n, n, len(R)))\n",
    "    for k in range(len(R)):\n",
    "        P[:, :, k] = dot(A, dot(R[k], A.T))\n",
    "    return P\n",
    "\n",
    "def parafac(data):\n",
    "    T = dtensor(data)\n",
    "    P = cp_als(T, RANK, init=\"random\")\n",
    "    U = [tensor(m) for m in P[0].U]\n",
    "    return kruskal_to_tensor(U)\n",
    "\n",
    "def tucker_(data):\n",
    "    T = dtensor(data)\n",
    "    core, U = tucker.hooi(T, RANK)\n",
    "    return tucker_to_tensor(core, U)\n",
    "\n",
    "def tensor_train(data):\n",
    "    with tf.Session() as sess:\n",
    "        t = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "        data_tt = t3f.to_tt_tensor(t, max_tt_rank=32)\n",
    "        #data_tt_round = t3f.round(data_tt, max_tt_rank=2)\n",
    "        P = t3f.full(data_tt)\n",
    "        P = sess.run(P)\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_predictions(P, e, k):\n",
    "    print(P.shape)\n",
    "    for a in range(e):\n",
    "        for b in range(e):\n",
    "            nrm = np.linalg.norm(P[a, b, :k])\n",
    "            if nrm != 0:\n",
    "                # round values for faster computation of AUC-PR\n",
    "                P[a, b, :k] = np.round_(P[a, b, :k] / nrm, decimals=3)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def innerfold(T, mask_idx, target_idx, e, k, sz, GROUND_TRUTH):\n",
    "    auc_methods = {}\n",
    "    Tc = [Ti.copy() for Ti in T]\n",
    "    mask_idx = np.unravel_index(mask_idx, (e, e, k))\n",
    "    target_idx = np.unravel_index(target_idx, (e, e, k))\n",
    "\n",
    "    # set values to be predicted to zero\n",
    "    for i in range(len(mask_idx[0])):\n",
    "        #print(Tc[mask_idx[2][i]][mask_idx[0][i], mask_idx[1][i]])\n",
    "        Tc[mask_idx[2][i]][mask_idx[0][i], mask_idx[1][i]] = 0\n",
    "\n",
    "    #print(len(mask_idx[0]))\n",
    "    #print(len(target_idx[0]))\n",
    "    GT = GROUND_TRUTH[target_idx]\n",
    "    print(len(GT))\n",
    "    # predict unknown values\n",
    "    # RESCAL\n",
    "    print(\"RESCAL decomposition...\")\n",
    "    P = predict_rescal_als(Tc)\n",
    "    P = normalize_predictions(P, e, k)\n",
    "    print(len(P[target_idx]))\n",
    "    print(\"Error: \", np.linalg.norm(GROUND_TRUTH-P))\n",
    "    prec, recall, _ = precision_recall_curve(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    #prec, recall, _, _ = precision_recall_fscore_support(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    #print(prec.shape)\n",
    "    avg_prec = average_precision_score(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    print(\"avg_p: \",avg_prec)\n",
    "    auc_methods['rescal'] = {'auc': auc(recall, prec),\n",
    "                             'prec': avg_prec,}\n",
    "                             #'recall': recall}\n",
    "    Tc_np = [t.toarray() for t in Tc]\n",
    "    Tc_np = np.array(Tc_np).T\n",
    "    \"\"\"\n",
    "    # tucker\n",
    "    print(\"Tucker decomposition...\")\n",
    "    P = tucker_(Tc_np)\n",
    "    P = normalize_predictions(P, e, k)\n",
    "    print(\"Error: \", np.linalg.norm(GROUND_TRUTH-P))\n",
    "    prec, recall, _ = precision_recall_curve(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    #prec, recall, _, _ = precision_recall_fscore_support(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    avg_prec = average_precision_score(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    print(\"avg_p: \",avg_prec)\n",
    "    auc_methods['tucker'] = {'auc': auc(recall, prec),\n",
    "                             'prec': avg_prec,}\n",
    "                             #'recall': recall}\n",
    "    # cp\n",
    "    print(\"Parafac decomposition...\")\n",
    "    P = parafac(Tc_np)\n",
    "    P = normalize_predictions(P, e, k)\n",
    "    print(\"Error: \", np.linalg.norm(GROUND_TRUTH-P))\n",
    "    prec, recall, _ = precision_recall_curve(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    #prec, recall, _, _ = precision_recall_fscore_support(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    avg_prec = average_precision_score(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    print(\"avg_p: \",avg_prec)\n",
    "    auc_methods['parafac'] = {'auc': auc(recall, prec),\n",
    "                              'prec': avg_prec,}\n",
    "                              #'recall': recall}\n",
    "    \n",
    "    # tensor train\n",
    "    print(\"Tensor Train decomposition...\")\n",
    "    P = tensor_train(Tc_np)\n",
    "    P = normalize_predictions(P, e, k)\n",
    "    print(\"Error: \", np.linalg.norm(GROUND_TRUTH-P))\n",
    "    prec, recall, _ = precision_recall_curve(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    #prec, recall, _, _ = precision_recall_fscore_support(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    avg_prec = average_precision_score(GROUND_TRUTH[target_idx], P[target_idx])\n",
    "    print(\"avg_p: \",avg_prec)\n",
    "    auc_methods['tt'] = {'auc': auc(recall, prec),\n",
    "                         'prec': avg_prec,}\n",
    "                         #'recall': recall}\n",
    "    \"\"\"\n",
    "\n",
    "    return auc_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_averages(results_dict):\n",
    "    methods = results_dict[0].keys()\n",
    "    averages = {}\n",
    "    for method in methods:\n",
    "        avprec = []\n",
    "        avrec = []\n",
    "        for f in results_dict.keys():\n",
    "            avprec.append(results_dict[f][method]['prec'])\n",
    "            #avrec.append(results_dict[f][method]['recall'])\n",
    "            #avprec.append(results_dict[f][method]['prec'])\n",
    "            #avrec.append(results_dict[f][method]['recall'])\n",
    "        averages[method] = {'avprec': avprec,}\n",
    "                            #'avrec': avrec}\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(recall, precision, m):\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve for '+str(m))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(results):\n",
    "    from itertools import cycle\n",
    "    lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "    linecycler = cycle(lines)\n",
    "    plt.figure()\n",
    "    for r in results.items():\n",
    "        x = list(range(len(r[1]['avprec'])))\n",
    "        #print(x)\n",
    "        y = r[1]['avprec']\n",
    "        plt.plot(x, y,next(linecycler),label=str(r[0]))\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xticks(x)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(multiarray_sparse, sample):\n",
    "    marrays = []\n",
    "    if sample is True:\n",
    "        sorted_m = sorted(multiarray_sparse, key=csr_matrix.getnnz)\n",
    "        for m in sorted_m[-10:]: #correct > [-10:]\n",
    "            marrays.append(m.toarray())\n",
    "        T = np.array(marrays).T\n",
    "    else:\n",
    "        for m in multiarray_sparse:\n",
    "            marrays.append(m.toarray())\n",
    "        T = np.array(marrays).T\n",
    "    return T, marrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  (645, 645, 10)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "\n",
    "def to_tensor(multiarray_sparse, sample):\n",
    "    marrays = []\n",
    "    if sample is True:\n",
    "        sorted_m = sorted(multiarray_sparse, key=csr_matrix.getnnz)\n",
    "        for m in sorted_m[-10:]: #correct > [-10:]\n",
    "            marrays.append(m.toarray())\n",
    "        T = np.array(marrays).T\n",
    "    else:\n",
    "        for m in multiarray_sparse:\n",
    "            marrays.append(m.toarray())\n",
    "        T = np.array(marrays).T\n",
    "    return T, marrays\n",
    "\n",
    "data, sparse_data = to_tensor(multiarray, sample=True)\n",
    "e, k = data.shape[0], data.shape[2]\n",
    "print(\"Dataset size: \", data.shape)\n",
    "SZ = e * e * k\n",
    "FOLDS = 10\n",
    "# T for rescal\n",
    "#T = [lil_matrix(data[:, :, i]) for i in range(k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_test = {}\n",
    "for f in range(FOLDS):\n",
    "    # test sets\n",
    "    # prepare indices for test values\n",
    "    ones_indices = np.nonzero(data)\n",
    "    zeros_indices = np.where(data == 0)\n",
    "    ones_ravel = np.ravel_multi_index(ones_indices, (e,e,k))\n",
    "    zeros_ravel = np.ravel_multi_index(zeros_indices, (e,e,k))\n",
    "    shuffle(ones_ravel)\n",
    "    shuffle(zeros_ravel)\n",
    "    random_ones_indices = ones_ravel[:int((np.count_nonzero(data)/50))]\n",
    "    random_zeros_indices = zeros_ravel[:int((np.count_nonzero(data)/50))]\n",
    "    test_indices = np.append(random_ones_indices, random_zeros_indices)\n",
    "    #print(test_indices.shape)\n",
    "    print('Test Fold %d' % f)\n",
    "    AUC_test[f] = innerfold(T, test_indices, test_indices, e, k, SZ, data)\n",
    "averages_methods = calculate_averages(AUC_test)\n",
    "#for m in averages_methods.items():\n",
    "#    plot_pr_curve(m[1]['avrec'], m[1]['avprec'], m[0])\n",
    "\n",
    "plot_all(averages_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classifier with pairs of drugs learned from latent factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to classify pairs of drugs, we can build a classifier using latent factors learned in the previous stages. To clarify, as training instances we use the concatenation(or addition) of two rows of the A latent matrix produced from RESCAL (each row is the latent representation of a drug). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_size=10000, test_size=2000):\n",
    "    test_indices = []\n",
    "    train_indices = []\n",
    "    all_ones = []\n",
    "    zeros_ind = []\n",
    "    n = dataset[0].shape[0]\n",
    "    all_possible_pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            all_possible_pairs.append((i,j))\n",
    "    #print((all_possible_pairs[:100]))\n",
    "    for adj_m in dataset:\n",
    "        nz = adj_m.nonzero()\n",
    "        nz_i = nz[0]\n",
    "        nz_j = nz[1]\n",
    "        indices = [(v, nz_j[i]) for i, v in enumerate(nz_i)]\n",
    "        #print(len(indices))\n",
    "        all_ones += indices\n",
    "        #print(all_ones)\n",
    "    #print(len(all_ones))\n",
    "    uniq_ones = list(set(all_ones))\n",
    "    zeros_ind = list(set(all_possible_pairs) - set(all_ones))\n",
    "    shuffle(all_ones)\n",
    "    shuffle(zeros_ind)\n",
    "    train_indices = all_ones[:train_size] + zeros_ind[:train_size]\n",
    "    test_indices = all_ones[train_size:train_size+test_size] + zeros_ind[train_size:train_size+test_size]\n",
    "    #print(len(train_indices))\n",
    "    #print(len(test_indices))\n",
    "    return train_indices, test_indices\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm \n",
    "\n",
    "avg_ac = []\n",
    "for f in range(FOLDS):\n",
    "    # test sets\n",
    "    # prepare indices for train and test\n",
    "    train_indices, test_indices = split_dataset(multiarray,\n",
    "                                                train_size=10000,\n",
    "                                                test_size=2000)\n",
    "    \n",
    "    # remove ones(test indices) from tensor\n",
    "    for pair in test_indices:\n",
    "        data[pair[0],pair[1], :] = 0\n",
    "        #both directions\n",
    "        data[pair[1],pair[0], :] = 0\n",
    "        \n",
    "    T = [lil_matrix(data[:, :, i]) for i in range(k)]\n",
    "    Tc = [Ti.copy() for Ti in T]\n",
    "    A = predict_rescal_als(Tc, isClass=True)\n",
    "    print('Fold %d' % f)\n",
    "    # prepare input for svm\n",
    "    X = []\n",
    "    y = []\n",
    "    half = int(len(train_indices)/2)\n",
    "    print(half)\n",
    "    for pair in train_indices[:half]:\n",
    "        X.append(np.append(A[pair[0]],A[pair[1]]))\n",
    "        y.append(1)\n",
    "    for pair in train_indices[half:]:\n",
    "        X.append(np.append(A[pair[0]],A[pair[1]]))\n",
    "        y.append(0)\n",
    "                 \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X, y)\n",
    "                 \n",
    "    #prepare indices for prediction\n",
    "    testX = []\n",
    "    testY = []\n",
    "    half = int(len(test_indices)/2)\n",
    "    for pair in test_indices[:half]:\n",
    "        testX.append(np.append(A[pair[0]],A[pair[1]]))\n",
    "        testY.append(1)\n",
    "    for pair in test_indices[half:]:\n",
    "        testX.append(np.append(A[pair[0]],A[pair[1]]))\n",
    "        testY.append(0)\n",
    "    predictions = clf.predict(testX)\n",
    "                     \n",
    "    from sklearn.metrics import precision_recall_curve, accuracy_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    precision, recall, _ = precision_recall_curve(testY, predictions)\n",
    "    acs = accuracy_score(testY, predictions)\n",
    "    avg_ac.append(acs)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.show()\n",
    "\n",
    "print(\"Average Accuracy after folds: \", sum(avg_ac)/FOLDS) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare with prediction of missing links we redesign the evaluation of classification task. Here we plan to exclude the pairs of drugs that will be used  as testing instances   from the factorization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of multiple classes - side effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we examine the case of classifying the type of side effects between two drugs.\n",
    "\n",
    "Let's see how many pairs of drugs exist for each side effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cse = []\n",
    "for se in sparse_data:\n",
    "    #print(\"Side effects sorted: \")\n",
    "    cse.append(np.count_nonzero(se))\n",
    "print(sorted(cse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are many side effects with few pairs. This would lead to an unbalanced training set for the task of multiclass classification. \n",
    "\n",
    "* First, we will use the latent representation of AiRi. So, each drug will have a representation wrt side effect. This way we will built a common binary svm model which will decide if a specific side effect is a result of a combination of drugs ((A_iR_i, A_jR_i) = 1 or 0)   \n",
    "* As a second attempt, we will split the dataset wrt the first top k side effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_multiclass(dataset, train_size=1000, test_size=100):\n",
    "    test_indices = []\n",
    "    train_indices = []\n",
    "    all_ones = []\n",
    "    zeros_ind = []\n",
    "    n = dataset[0].shape[0]\n",
    "    all_possible_pairs = []\n",
    "    y_test = []\n",
    "    y_train = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            all_possible_pairs.append((i,j))\n",
    "    for c, adj_m in enumerate(dataset):\n",
    "        nz = np.nonzero(adj_m)\n",
    "        nz_i = nz[0]\n",
    "        nz_j = nz[1]\n",
    "        ones_indices = [(v, nz_j[i]) for i, v in enumerate(nz_i)]\n",
    "        #print(len(indices))\n",
    "        #all_ones += indices\n",
    "        #print(len(all_ones))\n",
    "        #uniq_ones = list(set(ones_indices))\n",
    "        zeros_ind = list(set(all_possible_pairs) - set(ones_indices))\n",
    "        shuffle(ones_indices)\n",
    "        shuffle(zeros_ind)\n",
    "        train_indices += ones_indices[:train_size] + zeros_ind[:train_size]\n",
    "        test_indices += ones_indices[train_size:train_size+test_size] + zeros_ind[train_size:train_size+test_size]\n",
    "        y_train += [c for i in range(train_size*2)]\n",
    "        y_test += [c for i in range((test_size*2))]\n",
    "    #print(len(train_indices))\n",
    "    #print(len(test_indices))\n",
    "    #print(len(y_train))\n",
    "    #print((y_test[:1000]))\n",
    "    return train_indices, test_indices, y_train, y_test\n",
    "\n",
    "#split_dataset_multiclass(sparse_data, 1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "avg_ac = []\n",
    "for f in range(FOLDS):\n",
    "    # test sets\n",
    "    # prepare indices for train and test\n",
    "    train_indices, test_indices, y_train, y_test = split_dataset_multiclass(sparse_data,\n",
    "                                                train_size=1000,\n",
    "                                                test_size=1000)\n",
    "    \n",
    "    # remove ones(test indices) from tensor\n",
    "    #for pair in test_indices:\n",
    "    #    data[pair[0],pair[1], :] = 0\n",
    "        #both directions\n",
    "    #    data[pair[1],pair[0], :] = 0\n",
    "    classes = int((len(train_indices)/1000)/2) # divide by two as we train balanced (1, 0)\n",
    "    print(classes)\n",
    "    start = 0\n",
    "    sample = 2*1000\n",
    "    end = sample\n",
    "    \n",
    "    T = [lil_matrix(data[:, :, i]) for i in range(k)]\n",
    "    Tc = [Ti.copy() for Ti in T]\n",
    "    A, R = predict_rescal_als(Tc, isClass=True)\n",
    "    print('Fold %d' % f)\n",
    "    #print(len(train_indices))\n",
    "    # prepare input for svm\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(classes):\n",
    "        half = int(end - 1000)\n",
    "        for pair in train_indices[start:half]:\n",
    "            AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "            AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "            X.append(np.append(AR_1,AR_2))\n",
    "            y.append(1)\n",
    "        for pair in train_indices[half:end]:\n",
    "            AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "            AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "            X.append(np.append(AR_1,AR_2))\n",
    "            y.append(0)\n",
    "        start += sample\n",
    "        end += sample\n",
    "    \n",
    "    print(len(X))\n",
    "    print(len(y))\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X, y)\n",
    "                 \n",
    "    #prepare indices for prediction\n",
    "    testX = []\n",
    "    testY = []\n",
    "    start = 0\n",
    "    sample = 2*1000\n",
    "    end = sample\n",
    "    for i in range(classes):\n",
    "        half = int(end - 1000)\n",
    "        for pair in test_indices[start:half]:\n",
    "            AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "            AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "            testX.append(np.append(AR_1,AR_2))\n",
    "            testY.append(1)\n",
    "        for pair in test_indices[half:end]:\n",
    "            AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "            AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "            testX.append(np.append(AR_1,AR_2))\n",
    "            testY.append(0)\n",
    "        start += sample\n",
    "        end += sample\n",
    "    predictions = clf.predict(testX)\n",
    "    \n",
    "    from sklearn.metrics import precision_recall_curve, accuracy_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    precision, recall, _ = precision_recall_curve(predictions, testY)\n",
    "    acs = accuracy_score(testY, predictions)\n",
    "    avg_ac.append(acs)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "print(\"Average Accuracy after folds: \", sum(avg_ac)/FOLDS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a neural network\n",
    "\n",
    "We will train a simple neural network with 2 hidden layers, X inputs and c outputs (each corresponding to a class)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10000\n",
      "10000\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "(10000, 10)\n",
      "Step 1, Minibatch Loss= 0.0000, Training Accuracy= 0.000\n",
      "Step 100, Minibatch Loss= 0.0000, Training Accuracy= 0.000\n",
      "Step 200, Minibatch Loss= 0.0000, Training Accuracy= 0.000\n",
      "Step 300, Minibatch Loss= 0.0000, Training Accuracy= 0.000\n",
      "Step 400, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Step 500, Minibatch Loss= 0.0000, Training Accuracy= 0.000\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "train_indices, test_indices, y_train, y_test = split_dataset_multiclass(sparse_data,\n",
    "                                                train_size=1000,\n",
    "                                                test_size=1000)\n",
    "    \n",
    "# remove ones(test indices) from tensor\n",
    "#for pair in test_indices:\n",
    "#    data[pair[0],pair[1], :] = 0\n",
    "    #both directions\n",
    "#    data[pair[1],pair[0], :] = 0\n",
    "#print(10)\n",
    "classes_num = int((len(train_indices)/1000)/2) # divide by two as we train balanced (1, 0)\n",
    "print(classes_num)\n",
    "start = 0\n",
    "sample = 2*1000\n",
    "end = sample\n",
    "\n",
    "T = [lil_matrix(data[:, :, i]) for i in range(k)]\n",
    "Tc = [Ti.copy() for Ti in T]\n",
    "A, R = predict_rescal_als(Tc, isClass=True)\n",
    "#print(len(train_indices))\n",
    "# prepare input for svm\n",
    "X = []\n",
    "y = []\n",
    "y_1 = [0 for i in range(classes_num)]\n",
    "for i in range(classes_num):\n",
    "    half = int(end - 1000)\n",
    "    for pair in train_indices[start:half]:\n",
    "        AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "        AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "        X.append(np.append(AR_1,AR_2))\n",
    "        y_1[i] = 1\n",
    "        print(y_1)\n",
    "        y.append(y_1)\n",
    "        y_1[i] = 0\n",
    "    #for pair in train_indices[half:end]:\n",
    "    #    AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "    #    AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "    #2    X.append(np.append(AR_1,AR_2))\n",
    "    #    y_1[i] = 0\n",
    "    #    y.append(y_1)\n",
    "    #    y_1[i] = 0\n",
    "    start += sample\n",
    "    end += sample\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "\n",
    "y = np.array(y)\n",
    "print(y.shape)\n",
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 200 # 1st layer number of neurons\n",
    "n_hidden_2 = 200 # 2nd layer number of neurons\n",
    "num_input = len(X) # MNIST data input (img shape: 28*28)\n",
    "num_classes = classes_num # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X_nn = tf.placeholder(\"float\", [None, 200])\n",
    "Y_nn = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "\n",
    "# Store layers weight & bias# Store \n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model \n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Construct model# Const \n",
    "logits = neural_net(X_nn)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y_nn))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y_nn, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "# Start training# Start \n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X_nn: X, Y_nn: y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X_nn: X,\n",
    "                                                                 Y_nn: y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate all networks with node embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment we have examined only the drug-drug side effects network (645x645). However, there are two more networks available i) the protein-protein interaction network (19081x19081) and ii) the drug-protein interaction network (9569x9569). How to combine all this information together?\n",
    "\n",
    "1. we can concatenate all networks together, resulting in a huge tensor of size (645+19081+9569)x(645+19081+9569)x(2000+). Following this idea, we will come up with a very sparse tensor, for most of the relations in the 3rd dimension. (will probably lead to memory and computational time issues)\n",
    "2. we can use the p-p and d-p networks to compute a node embedding for each drug. This embedding will be used then as input to the factorizatio of d-d network. Why will this work?\n",
    "3. we can combine all networks to produce a node embedding for each type of node. Now, suppose we have a number of features for each (drug?) node. Is it possible to combine these together?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
