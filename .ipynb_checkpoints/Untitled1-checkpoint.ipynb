{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug-Drug Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(645, 645, 500)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "with open('./data/drug_drug_500.np', 'r') as f:\n",
    "    dd = np.load(f)\n",
    "print dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug-Protein Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(645, 3648)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "with open('./data/drug_protein_sparse.npz', 'r') as f:\n",
    "    drug_features = scipy.sparse.load_npz(f)\n",
    "drug_features = drug_features.toarray()\n",
    "print drug_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_multiclass(dataset, train_size=1000, test_size=100):\n",
    "    test_indices = []\n",
    "    train_indices = []\n",
    "    all_ones = []\n",
    "    zeros_ind = []\n",
    "    n = dataset[0].shape[0]\n",
    "    all_possible_pairs = []\n",
    "    y_test = []\n",
    "    y_train = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            all_possible_pairs.append((i,j))\n",
    "    for c, adj_m in enumerate(dataset):\n",
    "        nz = np.nonzero(adj_m)\n",
    "        nz_i = nz[0]\n",
    "        nz_j = nz[1]\n",
    "        ones_indices = [(v, nz_j[i]) for i, v in enumerate(nz_i)]\n",
    "        zeros_ind = list(set(all_possible_pairs) - set(ones_indices))\n",
    "        shuffle(ones_indices)\n",
    "        shuffle(zeros_ind)\n",
    "        train_indices += ones_indices[:train_size] + zeros_ind[:train_size]\n",
    "        test_indices += ones_indices[train_size:train_size+test_size] + zeros_ind[train_size:train_size+test_size]\n",
    "        y_train += [c for i in range(train_size*2)]\n",
    "        y_test += [c for i in range((test_size*2))]\n",
    "    return train_indices, test_indices, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "avg_ac = []\n",
    "for f in range(FOLDS):\n",
    "    # test sets\n",
    "    # prepare indices for train and test\n",
    "    train_indices, test_indices, y_train, y_test = split_dataset_multiclass(sparse_data,\n",
    "                                                train_size=1000,\n",
    "                                                test_size=1000)\n",
    "    \n",
    "    # remove ones(test indices) from tensor\n",
    "    #for pair in test_indices:\n",
    "    #    data[pair[0],pair[1], :] = 0\n",
    "        #both directions\n",
    "    #    data[pair[1],pair[0], :] = 0\n",
    "    classes = int((len(train_indices)/1000)/2) # divide by two as we train balanced (1, 0)\n",
    "    print(classes)\n",
    "    start = 0\n",
    "    sample = 2*1000\n",
    "    end = sample\n",
    "    \n",
    "    T = [lil_matrix(data[:, :, i]) for i in range(k)]\n",
    "    Tc = [Ti.copy() for Ti in T]\n",
    "    A, R = predict_rescal_als(Tc, isClass=True)\n",
    "    print('Fold %d' % f)\n",
    "    #print(len(train_indices))\n",
    "    # prepare input for svm\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(classes):\n",
    "        half = int(end - 1000)\n",
    "        for pair in train_indices[start:half]:\n",
    "            AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "            AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "            X.append(np.append(AR_1,AR_2))\n",
    "            y.append(1)\n",
    "        for pair in train_indices[half:end]:\n",
    "            AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "            AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "            X.append(np.append(AR_1,AR_2))\n",
    "            y.append(0)\n",
    "        start += sample\n",
    "        end += sample\n",
    "    \n",
    "    print(len(X))\n",
    "    print(len(y))\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X, y)\n",
    "                 \n",
    "    #prepare indices for prediction\n",
    "    testX = []\n",
    "    testY = []\n",
    "    start = 0\n",
    "    sample = 2*1000\n",
    "    end = sample\n",
    "    for i in range(classes):\n",
    "        half = int(end - 1000)\n",
    "        for pair in test_indices[start:half]:\n",
    "            AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "            AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "            testX.append(np.append(AR_1,AR_2))\n",
    "            testY.append(1)\n",
    "        for pair in test_indices[half:end]:\n",
    "            AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "            AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "            testX.append(np.append(AR_1,AR_2))\n",
    "            testY.append(0)\n",
    "        start += sample\n",
    "        end += sample\n",
    "    predictions = clf.predict(testX)\n",
    "    \n",
    "    from sklearn.metrics import precision_recall_curve, accuracy_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    precision, recall, _ = precision_recall_curve(predictions, testY)\n",
    "    acs = accuracy_score(testY, predictions)\n",
    "    avg_ac.append(acs)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "print(\"Average Accuracy after folds: \", sum(avg_ac)/FOLDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_data_to_multiarray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ba14f9a904ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/media/kostas/DATA/LLD/Papers/RESEARCH/GRAPH_CONV/data/bio-decagon-combo.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mdatafile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csvfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mmultiarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_data_to_multiarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_data_to_multiarray' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"In this code block, you must define the function `extract_main_dataset`.\n",
    "`extract_main_dataset` must take no arguments and return a tuple (X, y), where X is a\n",
    "Numpy array with shape (n_samples, n_features) corresponding to the features of your\n",
    "main dataset and y is the Numpy array corresponding to the ground truth labels of each\n",
    "sample.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "import pandas as pd\n",
    "from sktensor import rescal\n",
    "\n",
    "def predict_rescal_als(T, isClass=False):\n",
    "    A, R, _, _, _ = rescal.als(\n",
    "        T, 100, init='nvecs', conv=1e-3,\n",
    "        lambda_A=10, lambda_R=10\n",
    "    )\n",
    "    n = A.shape[0]\n",
    "    if isClass:\n",
    "        return A, R\n",
    "    P = zeros((n, n, len(R)))\n",
    "    for k in range(len(R)):\n",
    "        P[:, :, k] = dot(A, dot(R[k], A.T))\n",
    "    return P\n",
    "\n",
    "\n",
    "def split_dataset_multiclass(dataset, train_size=1000, test_size=100):\n",
    "    test_indices = []\n",
    "    train_indices = []\n",
    "    all_ones = []\n",
    "    zeros_ind = []\n",
    "    n = dataset[0].shape[0]\n",
    "    all_possible_pairs = []\n",
    "    y_test = []\n",
    "    y_train = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            all_possible_pairs.append((i,j))\n",
    "    for c, adj_m in enumerate(dataset):\n",
    "        nz = np.nonzero(adj_m)\n",
    "        nz_i = nz[0]\n",
    "        nz_j = nz[1]\n",
    "        ones_indices = [(v, nz_j[i]) for i, v in enumerate(nz_i)]\n",
    "        #print(len(indices))\n",
    "        #all_ones += indices\n",
    "        #print(len(all_ones))\n",
    "        #uniq_ones = list(set(ones_indices))\n",
    "        zeros_ind = list(set(all_possible_pairs) - set(ones_indices))\n",
    "        shuffle(ones_indices)\n",
    "        shuffle(zeros_ind)\n",
    "        train_indices += ones_indices[:train_size] + zeros_ind[:train_size]\n",
    "        test_indices += ones_indices[train_size:train_size+test_size] + zeros_ind[train_size:train_size+test_size]\n",
    "        y_train += [c for i in range(train_size*2)]\n",
    "        y_test += [c for i in range((test_size*2))]\n",
    "    #print(len(train_indices))\n",
    "    #print(len(test_indices))\n",
    "    #print(len(y_train))\n",
    "    #print((y_test[:1000]))\n",
    "    return train_indices, test_indices, y_train, y_test\n",
    "\n",
    "def load_csvfile(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "def convert_data_to_adjMatrix(data):\n",
    "    # TODO:: refactor how to get columns\n",
    "    columns = (data.iloc[:, i] for i in range(2))\n",
    "    df = pd.crosstab(*columns)\n",
    "    #df = pd.crosstab(data.a, data.b)\n",
    "    idx = df.columns.union(df.index)\n",
    "    adj_matrix = df.reindex(index=idx, columns=idx, fill_value=0)\n",
    "    return adj_matrix.values\n",
    "\n",
    "def convert_dataframe_to_sparse(matrix):\n",
    "    return sp.csr_matrix(matrix)\n",
    "\n",
    "def to_tensor(multiarray_sparse, sample):\n",
    "    marrays = []\n",
    "    if sample is True:\n",
    "        sorted_m = sorted(multiarray_sparse, key=csr_matrix.getnnz)\n",
    "        for m in sorted_m[-10:]: #correct > [-10:]\n",
    "            marrays.append(m.toarray())\n",
    "        T = np.array(marrays).T\n",
    "    else:\n",
    "        for m in multiarray_sparse:\n",
    "            marrays.append(m.toarray())\n",
    "        T = np.array(marrays).T\n",
    "    return T, marrays\n",
    "\n",
    "PATH = \"/media/kostas/DATA/LLD/Papers/RESEARCH/GRAPH_CONV/data/bio-decagon-combo.csv\"\n",
    "datafile = load_csvfile(PATH)\n",
    "multiarray = convert_data_to_multiarray(datafile)\n",
    "data, sparse_data = to_tensor(multiarray, sample=True)\n",
    "e, k = data.shape[0], data.shape[2]\n",
    "print(\"Dataset size: \", data.shape)\n",
    "train_indices, test_indices, y_train, y_test = split_dataset_multiclass(sparse_data,\n",
    "                                            train_size=1000,\n",
    "                                            test_size=1000)\n",
    "classes_num = int((len(train_indices)/1000)/2) # divide by two as we train balanced (1, 0)\n",
    "print(classes_num)\n",
    "start = 0\n",
    "sample = 2*1000\n",
    "end = sample\n",
    "T = [lil_matrix(data[:, :, i]) for i in range(k)]\n",
    "Tc = [Ti.copy() for Ti in T]\n",
    "A, R = predict_rescal_als(Tc, isClass=True)\n",
    "X = []\n",
    "y = []\n",
    "y_1 = [0 for i in range(classes_num)]\n",
    "for i in range(classes_num):\n",
    "    half = int(end - 1000)\n",
    "    for pair in train_indices[start:half]:\n",
    "        AR_1 = np.matmul(A[pair[0]], R[i])\n",
    "        AR_2 = np.matmul(A[pair[1]], R[i])\n",
    "        X.append(np.append(AR_1,AR_2))\n",
    "        y_1[i] = 1\n",
    "        print(y_1)\n",
    "        y.append(y_1)\n",
    "        y_1[i] = 0\n",
    "    start += sample\n",
    "    end += sample\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "y = np.array(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
